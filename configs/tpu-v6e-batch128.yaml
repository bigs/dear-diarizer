# WavLeJEPA training config for Google TPU v6e (Trillium)
# 32GB HBM per chip, 1600 GB/s bandwidth
# Note: TPU memory is managed automatically, no XLA_PYTHON_CLIENT_MEM_FRACTION needed
# Batch size should be multiple of 64 for optimal TPU utilization

precision:
  compute_dtype: bfloat16
  loss_in_float32: true

optimizer:
  peak_lr: 4.0e-4
  warmup_steps: 5000
  total_steps: 50000
  weight_decay: 0.04
  grad_clip_norm: 1.0

loss:
  sigreg_weight: 0.02
  num_slices: 256

data:
  batch_size: 128
  sample_rate: 16000
  crop_duration: 2.0

checkpoint:
  checkpoint_dir: checkpoints
  save_every_n_steps: 2500
  keep_n_checkpoints: 3

logging:
  project: wavlejepa
  log_every_n_steps: 50
  eval_every_n_steps: 500

seed: 42
