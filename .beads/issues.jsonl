{"id":"dear-2ic","title":"Implement LinearAttentionLayer (Mamba-2/RWKV-7 style)","description":"Implement Mamba2-based linear attention layer as an Equinox module, building on mamba2-jax.\n\n## Approach\nUse `mamba2-jax` (added as dependency) for the core SSD algorithm, wrap with Equinox for parameter management.\n\n## Components to Implement\n\n### 1. `RMSNorm` (eqx.Module)\nSimple RMS normalization layer.\n\n### 2. `Mamba2Layer` (eqx.Module)\nSingle Mamba2 layer wrapping `mamba2_jax.ssd.ssd_naive`.\n\nParameters to manage:\n- `in_proj`: Linear projection to (x, B, C, dt)\n- `conv1d`: Depthwise causal convolution\n- `dt_bias`: Time step bias [num_heads]\n- `A_log`: Log of A matrix [num_heads]  \n- `D`: Residual connection weight [num_heads]\n- `norm`: RMSNorm before output\n- `out_proj`: Output projection\n\nInterface:\n```python\ndef __call__(\n    self,\n    x: Float[Array, 'seq_len hidden_dim'],\n    *,\n    initial_state: Optional[Array] = None,\n    return_final_state: bool = False,\n) -\u003e Tuple[Float[Array, 'seq_len hidden_dim'], Optional[Array]]:\n```\n\n### 3. `Mamba2Block` (eqx.Module)\nPre-norm + Mamba2Layer + residual connection.\n\n### 4. `LinearAttentionStack` (eqx.Module)\nStack of Mamba2Blocks that contextualizes frame embeddings.\n\nInterface:\n```python\ndef __call__(\n    self,\n    x: Float[Array, 'seq_len input_dim'],\n) -\u003e Float[Array, 'seq_len hidden_dim']:\n```\n\n## Dependencies\n- `mamba2-jax` (pip package, already added)\n\n## Location\n`generator/mamba.py`\n\n## Parent Epic\ndear-adr","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-15T10:24:07.875912-05:00","created_by":"Cole Brown","updated_at":"2026-01-15T10:56:43.855309-05:00","closed_at":"2026-01-15T10:56:43.855309-05:00","close_reason":"Closed","labels":["task"]}
{"id":"dear-87q","title":"Implement AttractorGenerator main module","description":"Implement the main AttractorGenerator module that orchestrates all components.\n\n## Requirements\n- Integrate LinearAttentionStack, CrossAttention, GRU\n- Implement iterative generation with jax.lax.while_loop\n- Confidence-based early stopping\n- Learned start token for step 0\n- Output: padded attractors array + confidences + valid_count\n\n## Key Implementation Details\n- GRU input at each step: [prev_attractor; cross_attn_output]\n- Step 0 uses learned start_token instead of prev_attractor\n- Hidden state initialized from mean-pooled contextualized frames\n- Generate until confidence \u003c threshold OR max_attractors reached\n\n## Interface\n```python\nclass AttractorGenerator(eqx.Module):\n    def __call__(self, frame_embeddings: Array, *, key) -\u003e tuple[Array, Array, Array]:\n        \"\"\"\n        Args:\n            frame_embeddings: [N, input_dim] from frozen WavLeJEPA\n        Returns:\n            attractors: [max_attractors, attractor_dim]\n            confidences: [max_attractors]\n            valid_count: scalar\n        \"\"\"\n```\n\n## Dependencies\n- LinearAttentionStack\n- CrossAttention\n\n## Parent Epic\ndear-adr","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-15T10:24:44.239087-05:00","created_by":"Cole Brown","updated_at":"2026-01-15T11:07:06.496105-05:00","closed_at":"2026-01-15T11:07:06.496105-05:00","close_reason":"Closed","labels":["task"]}
{"id":"dear-9fx","title":"Epic: Implement Gated DeltaNet in JAX","status":"open","priority":2,"issue_type":"epic","owner":"bigswim@gmail.com","created_at":"2026-01-16T10:04:06.271336-05:00","created_by":"Cole Brown","updated_at":"2026-01-16T10:04:06.271336-05:00"}
{"id":"dear-adr","title":"Epic: Implement Speaker Attractor Generator","description":"Implement the Generator component as specified in docs/generator_design.md. This is the second stage of the speaker diarization pipeline that takes WavLeJEPA frame embeddings and produces speaker attractors.\n\n## Key Components\n1. Linear Attention Stack (Mamba-2/RWKV-7 style)\n2. Cross-Attention Module\n3. GRU-based Recurrent Generator\n4. Energy-based training objective\n5. Test-time optimization\n\n## Architecture Flow\n```\nWavLeJEPA (frozen) → Linear Attention Stack → Contextualized Frames\n                                                    ↓\n                                              Cross-Attention ← GRU hidden\n                                                    ↓\n                                              GRU → attractor + confidence\n                                              (loop until confidence \u003c θ)\n```\n\nSee docs/generator_design.md for full specification.","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-15T10:23:03.102164-05:00","created_by":"Cole Brown","updated_at":"2026-01-15T17:32:53.640306-05:00","closed_at":"2026-01-15T17:32:53.640306-05:00","close_reason":"Closed","labels":["epic"]}
{"id":"dear-bwn","title":"Create synthetic multi-speaker test data","description":"Create synthetic test data for developing and testing the Generator.\n\n## Requirements\n- Generate fake 'frame embeddings' that simulate multi-speaker audio\n- Should have clear cluster structure (each speaker = distinct cluster)\n- Variable number of speakers (1-10)\n- Variable segment lengths per speaker\n- Should be reproducible (seeded RNG)\n\n## Use Cases\n- Unit testing energy functions\n- Integration testing full generator\n- Debugging training loop\n- Visualizing attractor quality\n\n## Suggested Approach\n```python\ndef make_synthetic_data(\n    num_speakers: int,\n    frames_per_speaker: int,\n    embedding_dim: int = 768,\n    noise_scale: float = 0.1,\n    key: PRNGKey\n) -\u003e tuple[Array, Array]:\n    \"\"\"\n    Returns:\n        frame_embeddings: [N, D] where N = num_speakers * frames_per_speaker\n        speaker_labels: [N] ground truth (for evaluation only)\n    \"\"\"\n```\n\n## Parent Epic\ndear-adr","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-15T10:25:30.054594-05:00","created_by":"Cole Brown","updated_at":"2026-01-15T11:24:55.622556-05:00","closed_at":"2026-01-15T11:24:55.622556-05:00","close_reason":"Closed","labels":["task"]}
{"id":"dear-ile","title":"Implement test-time optimization loop","description":"Implement attractor refinement via energy minimization at inference.\n\n## Specification\n```python\ndef refine_attractors(A_init, X, num_steps=50, lr=0.01):\n    A = A_init\n    for _ in range(num_steps):\n        grad_A = grad(E, argnums=0)(A, X)\n        A = A - lr * grad_A\n    return A\n```\n\n## Requirements\n- Gradient descent on energy w.r.t. attractors only\n- Configurable: num_steps, learning_rate\n- Consider stopping criteria based on energy convergence\n- May need attractor re-normalization after gradient steps\n- Must handle masking for invalid attractors\n\n## Parent Epic\ndear-adr","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-15T10:25:14.661784-05:00","created_by":"Cole Brown","updated_at":"2026-01-15T11:30:46.039708-05:00","closed_at":"2026-01-15T11:30:46.039708-05:00","close_reason":"Closed","labels":["task"]}
{"id":"dear-kgw","title":"Implement energy functions","description":"Implement the energy-based objective functions for training.\n\n## Energy Components\n\n### E_assignment (Frame-to-Attractor)\n```python\nd_ik = ||x_i - a_k||²                 # L2 squared distance\nw_ik = softmax(-d_ik / τ)            # soft assignment (softmin)\nE_assignment = (1/N) * Σ_i Σ_k w_ik * d_ik\n```\n\n### E_separation (Attractor Diversity)\n```python\nE_separation = Σ_{k≠j} max(0, margin - ||a_k - a_j||)\n```\nHinge loss with configurable margin.\n\n### E_coverage (No Orphan Frames) - Optional\n```python\nusage_k = Σ_i w_ik\nE_coverage = Σ_k max(0, min_usage - usage_k)\n```\n\n## Combined Energy\n```python\nE(A, X) = E_assignment + λ_sep * E_separation + λ_cov * E_coverage\n```\n\n## Requirements\n- All functions must be JAX-differentiable\n- Temperature τ should be a parameter (for annealing)\n- Must handle variable number of valid attractors (masking)\n\n## Parent Epic\ndear-adr","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-15T10:24:56.402514-05:00","created_by":"Cole Brown","updated_at":"2026-01-15T11:12:49.924457-05:00","closed_at":"2026-01-15T11:12:49.924457-05:00","close_reason":"Closed","labels":["task"]}
{"id":"dear-mfk","title":"Implement CrossAttention module","description":"Implement multi-head cross-attention for querying contextualized frames from GRU hidden state.\n\n## Requirements\n- Multi-head attention (configurable num_heads)\n- Query: GRU hidden state [D]\n- Key/Value: contextualized frames [N, D]\n- Output: attended context vector [D]\n\n## Interface\n```python\nclass CrossAttention(eqx.Module):\n    num_heads: int\n    head_dim: int\n    w_q: eqx.nn.Linear\n    w_k: eqx.nn.Linear\n    w_v: eqx.nn.Linear\n    w_o: eqx.nn.Linear\n    \n    def __call__(self, query: Array, kv: Array) -\u003e Array:\n        \"\"\"\n        Args:\n            query: [D] GRU hidden state\n            kv: [N, D] contextualized frames\n        Returns:\n            [D] attended context vector\n        \"\"\"\n```\n\n## Parent Epic\ndear-adr","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-15T10:24:29.164819-05:00","created_by":"Cole Brown","updated_at":"2026-01-15T11:01:04.73503-05:00","closed_at":"2026-01-15T11:01:04.73503-05:00","close_reason":"Closed","labels":["task"]}
{"id":"dear-mhy","title":"Implement LinearAttentionStack","description":"Implement the stack of gated linear attention layers that contextualizes frame embeddings.\n\n## Requirements\n- Stack 4-6 LinearAttentionLayer modules\n- Configurable via GeneratorConfig.num_layers\n- Maintains sequence length (no pooling)\n\n## Interface\n```python\nclass LinearAttentionStack(eqx.Module):\n    layers: list[LinearAttentionLayer]\n    \n    def __call__(self, x: Array) -\u003e Array:\n        \"\"\"Process frame embeddings through all layers.\n        \n        Args:\n            x: [N, D] frame embeddings\n        Returns:\n            [N, D] contextualized frame representations\n        \"\"\"\n```\n\n## Dependencies\n- Requires LinearAttentionLayer implementation\n\n## Parent Epic\ndear-adr","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-15T10:24:18.737134-05:00","created_by":"Cole Brown","updated_at":"2026-01-15T10:58:01.628143-05:00","closed_at":"2026-01-15T10:58:01.628143-05:00","close_reason":"Closed","labels":["task"]}
{"id":"dear-ngw","title":"Implement GeneratorConfig dataclass","description":"Implement the configuration dataclass for the Generator.\n\n## Fields\n```python\n@dataclass\nclass GeneratorConfig:\n    # Dimensions\n    input_dim: int = 768          # WavLeJEPA output dim\n    hidden_dim: int = 768         # GRU hidden size\n    attractor_dim: int = 768      # Output attractor dimension\n    \n    # Linear attention stack\n    num_layers: int = 4           # Number of linear attention layers\n    \n    # Cross-attention\n    num_heads: int = 8            # Multi-head attention heads\n    \n    # Generation\n    max_attractors: int = 10      # Maximum speakers\n    confidence_threshold: float = 0.5\n    \n    # Energy weights\n    lambda_separation: float = 1.0\n    lambda_coverage: float = 0.1\n    separation_margin: float = 1.0\n    \n    # Temperature annealing\n    tau_start: float = 1.0\n    tau_end: float = 0.1\n    \n    # Confidence training\n    usage_threshold: float = 0.5  # seconds\n```\n\n## Parent Epic\ndear-adr","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-15T10:25:19.793439-05:00","created_by":"Cole Brown","updated_at":"2026-01-15T10:59:37.268067-05:00","closed_at":"2026-01-15T10:59:37.268067-05:00","close_reason":"Closed","labels":["task"]}
{"id":"dear-wk2","title":"Set up training loop with temperature annealing","description":"Implement the training loop for the Generator with deterministic annealing.\n\n## Requirements\n- Training objective: L = E(A, X) + λ_conf * L_confidence\n- Temperature annealing: τ goes from tau_start (soft) to tau_end (hard) over training\n- Frozen WavLeJEPA encoder (no gradients)\n- Trainable: LinearAttentionStack, CrossAttention, GRU, output heads\n\n## Training Flow\n1. Encode audio with frozen WavLeJEPA\n2. Pass through linear attention stack\n3. Generate attractors with GRU\n4. Compute energy loss + confidence loss\n5. Backprop through generator only\n\n## Considerations\n- Learning rate schedule\n- Gradient clipping\n- Checkpointing\n- Logging (loss components, attractor counts, etc.)\n\n## Parent Epic\ndear-adr","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-15T10:25:24.926136-05:00","created_by":"Cole Brown","updated_at":"2026-01-15T17:27:46.727641-05:00","closed_at":"2026-01-15T17:27:46.727641-05:00","close_reason":"Closed","labels":["task"]}
{"id":"dear-yne","title":"Implement confidence head training loss","description":"Implement the auxiliary usage-based confidence loss.\n\n## Specification\n```python\nusage_k = Σ_i w_ik                           # total soft-assignment mass\ntarget_k = 1 if usage_k \u003e threshold else 0   # threshold ≈ 0.5s audio\nL_confidence = (1/K) * Σ_k BCE(c_k, target_k)\n```\n\n## Requirements\n- Binary cross-entropy loss\n- Usage threshold configurable (default: frames equivalent to 0.5s audio)\n- Must handle masking for invalid attractors\n- Ensures confidence is self-consistent with energy function\n\n## Dependencies\n- Soft assignment weights from energy function (w_ik)\n\n## Parent Epic\ndear-adr","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-15T10:25:09.531043-05:00","created_by":"Cole Brown","updated_at":"2026-01-15T11:24:50.546459-05:00","closed_at":"2026-01-15T11:24:50.546459-05:00","close_reason":"Closed","labels":["task"]}
