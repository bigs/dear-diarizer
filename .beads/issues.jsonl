{"id":"dear-207","title":"Set up jax-gated-deltanet module structure and GatedDeltaNetConfig","description":"Create jax-gated-deltanet/ directory with __init__.py, config.py (GatedDeltaNetConfig dataclass with head_k_dim, head_v_dim, expand_v, num_heads, num_v_heads, conv_size, use_gate, use_short_conv, etc.)","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-16T10:14:54.868036-05:00","created_by":"Cole Brown","updated_at":"2026-01-16T10:21:31.186741-05:00","closed_at":"2026-01-16T10:21:31.186741-05:00","close_reason":"Closed"}
{"id":"dear-2ic","title":"Implement LinearAttentionLayer (Mamba-2/RWKV-7 style)","description":"Implement Mamba2-based linear attention layer as an Equinox module, building on mamba2-jax.\n\n## Approach\nUse `mamba2-jax` (added as dependency) for the core SSD algorithm, wrap with Equinox for parameter management.\n\n## Components to Implement\n\n### 1. `RMSNorm` (eqx.Module)\nSimple RMS normalization layer.\n\n### 2. `Mamba2Layer` (eqx.Module)\nSingle Mamba2 layer wrapping `mamba2_jax.ssd.ssd_naive`.\n\nParameters to manage:\n- `in_proj`: Linear projection to (x, B, C, dt)\n- `conv1d`: Depthwise causal convolution\n- `dt_bias`: Time step bias [num_heads]\n- `A_log`: Log of A matrix [num_heads]  \n- `D`: Residual connection weight [num_heads]\n- `norm`: RMSNorm before output\n- `out_proj`: Output projection\n\nInterface:\n```python\ndef __call__(\n    self,\n    x: Float[Array, 'seq_len hidden_dim'],\n    *,\n    initial_state: Optional[Array] = None,\n    return_final_state: bool = False,\n) -\u003e Tuple[Float[Array, 'seq_len hidden_dim'], Optional[Array]]:\n```\n\n### 3. `Mamba2Block` (eqx.Module)\nPre-norm + Mamba2Layer + residual connection.\n\n### 4. `LinearAttentionStack` (eqx.Module)\nStack of Mamba2Blocks that contextualizes frame embeddings.\n\nInterface:\n```python\ndef __call__(\n    self,\n    x: Float[Array, 'seq_len input_dim'],\n) -\u003e Float[Array, 'seq_len hidden_dim']:\n```\n\n## Dependencies\n- `mamba2-jax` (pip package, already added)\n\n## Location\n`generator/mamba.py`\n\n## Parent Epic\ndear-adr","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-15T10:24:07.875912-05:00","created_by":"Cole Brown","updated_at":"2026-01-15T10:56:43.855309-05:00","closed_at":"2026-01-15T10:56:43.855309-05:00","close_reason":"Closed","labels":["task"]}
{"id":"dear-2x8","title":"Configurable SSM backend (Mamba2 vs GatedDeltaNet)","status":"closed","priority":2,"issue_type":"epic","owner":"bigswim@gmail.com","created_at":"2026-01-16T11:23:53.162011-05:00","created_by":"Cole Brown","updated_at":"2026-01-16T11:31:51.712185-05:00","closed_at":"2026-01-16T11:31:51.712185-05:00","close_reason":"Closed"}
{"id":"dear-38t","title":"VoxCeleb1 calibration EER eval (1A)","description":"Implement a fast, repeatable speaker-verification calibration eval using VoxCeleb1 test trials. Scope: load trial pairs, extract embeddings with current WavLeJEPA probe path, L2-normalize, cosine score, compute EER, and write JSON results + CLI usage.","acceptance_criteria":"CLI runs end-to-end on GPU box; outputs EER for VoxCeleb1 test pairs; results saved to JSON with config metadata; brief README usage note.","status":"open","priority":3,"issue_type":"epic","owner":"bigswim@gmail.com","created_at":"2026-01-20T21:26:45.051781-05:00","created_by":"Cole Brown","updated_at":"2026-01-20T22:47:53.183792-05:00","labels":["eval","sv","voxceleb"]}
{"id":"dear-38t.1","title":"Acquire/parse VoxCeleb1 verification trials","description":"Add code to load VoxCeleb1 verification trial list (veri_test.txt) and map relative paths to absolute wav files under voxceleb root. Produce a manifest of unique files to embed and validate missing files with clear errors.","acceptance_criteria":"Parser loads trial list; resolves file paths under a configurable root; reports missing files; unit smoke test or CLI check confirms parsed counts.","status":"open","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-20T21:44:06.204396-05:00","created_by":"Cole Brown","updated_at":"2026-01-20T21:44:06.204396-05:00","labels":["eval","sv","voxceleb"],"dependencies":[{"issue_id":"dear-38t.1","depends_on_id":"dear-38t","type":"parent-child","created_at":"2026-01-20T21:44:06.20647-05:00","created_by":"Cole Brown"}]}
{"id":"dear-38t.2","title":"Embedding extraction for verification set","description":"Implement extraction/caching for VoxCeleb1 verification files using current WavLeJEPA probe path (Top-K + length-aware pooling). Batch extraction and save embeddings to disk (e.g., npy) with a manifest for reuse.","acceptance_criteria":"CLI or function extracts embeddings for all unique trial files; caches embeddings to disk; re-runs skip existing cache or overwrite with flag.","status":"open","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-20T21:44:11.909914-05:00","created_by":"Cole Brown","updated_at":"2026-01-20T21:44:11.909914-05:00","labels":["eval","sv","voxceleb"],"dependencies":[{"issue_id":"dear-38t.2","depends_on_id":"dear-38t","type":"parent-child","created_at":"2026-01-20T21:44:11.91185-05:00","created_by":"Cole Brown"}]}
{"id":"dear-38t.3","title":"Cosine scoring + EER computation","description":"Implement cosine similarity scoring for verification trials with L2-normalized embeddings and compute EER from the scores/labels. Provide a reusable utility function that returns EER and thresholds.","acceptance_criteria":"Given embeddings + trial list, code computes cosine scores and EER; includes simple test or sanity check with synthetic data.","status":"open","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-20T21:44:16.302962-05:00","created_by":"Cole Brown","updated_at":"2026-01-20T21:44:16.302962-05:00","labels":["eval","sv","voxceleb"],"dependencies":[{"issue_id":"dear-38t.3","depends_on_id":"dear-38t","type":"parent-child","created_at":"2026-01-20T21:44:16.30501-05:00","created_by":"Cole Brown"}]}
{"id":"dear-38t.4","title":"Calibration CLI + JSON results","description":"Wire an end-to-end CLI (e.g., evals/voxceleb/verify.py) that loads trials, extracts embeddings (or uses cache), scores, computes EER, and writes a JSON results file with config metadata (checkpoint, root, cache path, etc.).","acceptance_criteria":"CLI runs end-to-end on GPU box; outputs JSON with EER + config fields; prints a concise summary to stdout.","status":"open","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-20T21:44:20.876094-05:00","created_by":"Cole Brown","updated_at":"2026-01-20T21:44:20.876094-05:00","labels":["eval","sv","voxceleb"],"dependencies":[{"issue_id":"dear-38t.4","depends_on_id":"dear-38t","type":"parent-child","created_at":"2026-01-20T21:44:20.878411-05:00","created_by":"Cole Brown"}]}
{"id":"dear-38t.5","title":"Usage notes for calibration eval","description":"Add brief docs (README or evals/voxceleb/README.md) describing required VoxCeleb1 files, trial list source, expected directory structure, and CLI usage for the calibration EER eval.","acceptance_criteria":"Docs explain trial list location, required file structure, and example command; includes output example snippet.","status":"open","priority":3,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-20T21:44:26.347199-05:00","created_by":"Cole Brown","updated_at":"2026-01-20T21:44:26.347199-05:00","labels":["docs","eval","voxceleb"],"dependencies":[{"issue_id":"dear-38t.5","depends_on_id":"dear-38t","type":"parent-child","created_at":"2026-01-20T21:44:26.349368-05:00","created_by":"Cole Brown"}]}
{"id":"dear-3h3","title":"Add tests for SSM backend selection","description":"Test both backends produce valid output. Test config validation (both present = error, neither = error). Test YAML loading.","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-16T11:24:03.917714-05:00","created_by":"Cole Brown","updated_at":"2026-01-16T11:31:51.696774-05:00","closed_at":"2026-01-16T11:31:51.696774-05:00","close_reason":"Closed","dependencies":[{"issue_id":"dear-3h3","depends_on_id":"dear-2x8","type":"blocks","created_at":"2026-01-16T11:24:09.69813-05:00","created_by":"Cole Brown"},{"issue_id":"dear-3h3","depends_on_id":"dear-eba","type":"blocks","created_at":"2026-01-16T11:24:10.19108-05:00","created_by":"Cole Brown"}]}
{"id":"dear-3wh","title":"Epic: Chunkwise parallel gated delta rule (pure JAX)","description":"Implement chunkwise parallel algorithm for gated delta rule in pure JAX. This is Phase 1 before Pallas kernels - validates the algorithm and gives 2-5x speedup over naive scan.\n\nApproach:\n- Port FLA's chunk algorithm to JAX\n- Use jax.lax.associative_scan for inter-chunk state propagation\n- Add as alternative to naive scan (don't replace)\n- Validate equivalence with tests before switching default\n\nReference: fla-org/flash-linear-attention chunk.py\n\nSubtasks:\n- dear-gnt: Implement chunk utilities and WY representation\n- dear-ekt: Implement intra-chunk delta rule computation (depends on gnt)\n- dear-7ac: Implement inter-chunk state propagation (depends on gnt)\n- dear-ycg: Wire up chunkwise function (depends on ekt, 7ac)\n- dear-lsn: Add equivalence tests (depends on ycg)","status":"closed","priority":2,"issue_type":"epic","owner":"bigswim@gmail.com","created_at":"2026-01-16T10:59:33.95471-05:00","created_by":"Cole Brown","updated_at":"2026-01-16T11:13:36.114915-05:00","closed_at":"2026-01-16T11:13:36.114915-05:00","close_reason":"Closed"}
{"id":"dear-4ua","title":"Update existing YAML configs","description":"Migrate configs/ files to use nested mamba2: section instead of flat params.","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-16T11:24:03.622606-05:00","created_by":"Cole Brown","updated_at":"2026-01-16T11:29:30.551308-05:00","closed_at":"2026-01-16T11:29:30.551308-05:00","close_reason":"Closed","dependencies":[{"issue_id":"dear-4ua","depends_on_id":"dear-2x8","type":"blocks","created_at":"2026-01-16T11:24:09.634365-05:00","created_by":"Cole Brown"},{"issue_id":"dear-4ua","depends_on_id":"dear-pak","type":"blocks","created_at":"2026-01-16T11:24:10.131057-05:00","created_by":"Cole Brown"}]}
{"id":"dear-5py","title":"Set up evals/ directory structure","description":"Create evals/ subfolder with:\n- evals/__init__.py\n- evals/voxceleb/__init__.py\n- evals/voxceleb/probe.py (main logic)\n- evals/voxceleb/data.py (VoxCeleb data loading)\n\nKeep evaluation code separate from main training code.","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-16T17:54:55.059904-05:00","created_by":"Cole Brown","updated_at":"2026-01-17T12:30:22.828972-05:00","closed_at":"2026-01-17T12:30:22.828972-05:00","close_reason":"Created evals/ directory structure with __init__ files and stub implementations for data.py and probe.py"}
{"id":"dear-6ym","title":"Implement naive gated_delta_rule recurrence","description":"Implement deltanet.py with gated_delta_rule() using jax.lax.scan. Core math: h = h * exp(g); error = beta * (v - h @ k); h = h + outer(k, error); o = h @ q. This is the Pallas swap-in point.","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-16T10:14:55.750787-05:00","created_by":"Cole Brown","updated_at":"2026-01-16T10:23:09.829693-05:00","closed_at":"2026-01-16T10:23:09.829693-05:00","close_reason":"Closed","dependencies":[{"issue_id":"dear-6ym","depends_on_id":"dear-207","type":"blocks","created_at":"2026-01-16T10:15:11.734312-05:00","created_by":"Cole Brown"}]}
{"id":"dear-7ac","title":"Implement inter-chunk state propagation","description":"Propagate states between chunks using associative scan or matrix multiply.\n\n- Compute final state of each chunk (assuming zero initial)\n- Use jax.lax.associative_scan or segsum for inter-chunk decay\n- Combine chunk states with proper decay factors\n- Add contribution from initial states to outputs","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-16T10:59:57.060694-05:00","created_by":"Cole Brown","updated_at":"2026-01-16T11:13:36.075251-05:00","closed_at":"2026-01-16T11:13:36.075251-05:00","close_reason":"Closed","dependencies":[{"issue_id":"dear-7ac","depends_on_id":"dear-gnt","type":"blocks","created_at":"2026-01-16T11:00:13.477557-05:00","created_by":"Cole Brown"}]}
{"id":"dear-7jj","title":"VoxCeleb1 O/E/H SOTA eval suite (2A)","description":"Extend speaker-verification eval to official VoxCeleb1 protocols O/E/H. Scope: ingest protocol trial lists, run scoring pipeline, report EER (and optionally minDCF), and aggregate results in JSON + table.","acceptance_criteria":"O/E/H protocols runnable from CLI; outputs per-protocol metrics to JSON; README/doc notes the standard evaluation settings.","status":"open","priority":3,"issue_type":"epic","owner":"bigswim@gmail.com","created_at":"2026-01-20T21:26:49.973296-05:00","created_by":"Cole Brown","updated_at":"2026-01-20T22:48:08.409744-05:00","labels":["eval","sota","sv","voxceleb"]}
{"id":"dear-7jj.1","title":"Add minDCF to VoxCeleb O/E/H eval","description":"Implement minDCF computation alongside EER in the speaker-verification eval. Make priors/costs configurable via CLI (with sensible defaults) and include them in JSON output for reproducibility.","acceptance_criteria":"CLI supports minDCF params; JSON output includes EER + minDCF per protocol with recorded priors/costs; docs mention defaults.","status":"open","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-20T21:30:14.459658-05:00","created_by":"Cole Brown","updated_at":"2026-01-20T21:30:14.459658-05:00","labels":["eval","sv","voxceleb"],"dependencies":[{"issue_id":"dear-7jj.1","depends_on_id":"dear-7jj","type":"parent-child","created_at":"2026-01-20T21:30:14.461625-05:00","created_by":"Cole Brown"}]}
{"id":"dear-7xk","title":"Fix target ratio undershoot in masking","description":"## Problem\n\nTarget masking undershoots the configured ratio (~18% actual vs 23% target) because the overlap-aware formula doesn't account for context exclusion.\n\n### Current behavior\n\n```python\nnum_blocks = _num_blocks_for_ratio(\n    seq_len, config.target_ratio, config.target_block_length\n)\n```\n\nThe formula assumes blocks can be placed anywhere in the sequence, but:\n1. Block starts are excluded from context positions (via `-inf` masking)\n2. Expanded blocks get trimmed by `\u0026 ~context_mask`\n\n### Actual ratios observed\n\n| | Target | Actual |\n|--|--------|--------|\n| Context | 35% | ~37% ✓ |\n| Target | 23% | ~18% ✗ |\n\n## Proposed solution\n\nAdjust the target block calculation to account for available (non-context) space:\n\n```python\ndef sample_target_mask(...):\n    # Compute effective sequence length (non-context positions)\n    non_context_len = seq_len - int(seq_len * config.context_ratio)\n    \n    # Compute blocks needed for the available space\n    num_blocks = _num_blocks_for_ratio(\n        non_context_len, \n        config.target_ratio / (1 - config.context_ratio),  # Adjust ratio for smaller space\n        config.target_block_length\n    )\n```\n\nOr simpler: just bump the target_ratio default to compensate (~0.28 to hit actual ~23%).\n\n## Acceptance criteria\n\n- [ ] Actual target ratio within ±2% of configured ratio\n- [ ] Context ratio unchanged (~35%)\n- [ ] Tests verify both ratios hit targets","status":"open","priority":2,"issue_type":"bug","owner":"cole@kiosk.app","created_at":"2026-01-23T02:46:10.834529482Z","created_by":"Cole Brown","updated_at":"2026-01-23T02:46:10.834529482Z"}
{"id":"dear-87q","title":"Implement AttractorGenerator main module","description":"Implement the main AttractorGenerator module that orchestrates all components.\n\n## Requirements\n- Integrate LinearAttentionStack, CrossAttention, GRU\n- Implement iterative generation with jax.lax.while_loop\n- Confidence-based early stopping\n- Learned start token for step 0\n- Output: padded attractors array + confidences + valid_count\n\n## Key Implementation Details\n- GRU input at each step: [prev_attractor; cross_attn_output]\n- Step 0 uses learned start_token instead of prev_attractor\n- Hidden state initialized from mean-pooled contextualized frames\n- Generate until confidence \u003c threshold OR max_attractors reached\n\n## Interface\n```python\nclass AttractorGenerator(eqx.Module):\n    def __call__(self, frame_embeddings: Array, *, key) -\u003e tuple[Array, Array, Array]:\n        \"\"\"\n        Args:\n            frame_embeddings: [N, input_dim] from frozen WavLeJEPA\n        Returns:\n            attractors: [max_attractors, attractor_dim]\n            confidences: [max_attractors]\n            valid_count: scalar\n        \"\"\"\n```\n\n## Dependencies\n- LinearAttentionStack\n- CrossAttention\n\n## Parent Epic\ndear-adr","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-15T10:24:44.239087-05:00","created_by":"Cole Brown","updated_at":"2026-01-15T11:07:06.496105-05:00","closed_at":"2026-01-15T11:07:06.496105-05:00","close_reason":"Closed","labels":["task"]}
{"id":"dear-8vd","title":"Implement VoxCeleb data loading","description":"Implement data loading for VoxCeleb1 test set.\n\n## Requirements\n- Download/locate VoxCeleb1 test split\n- Parse speaker IDs from directory structure (id10001, id10002, etc.)\n- Load audio files, resample to 16kHz\n- Return (waveform, speaker_id) pairs\n\n## Interface\n```python\ndef load_voxceleb1_test(root: Path) -\u003e list[tuple[Path, int]]:\n    \"\"\"Returns list of (audio_path, speaker_id) pairs.\"\"\"\n```\n\nConsider using HuggingFace datasets if available.","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-16T17:54:58.355263-05:00","created_by":"Cole Brown","updated_at":"2026-01-17T12:30:47.873937-05:00","closed_at":"2026-01-17T12:30:47.873937-05:00","close_reason":"Implemented VoxCeleb data loading with speaker ID mapping","dependencies":[{"issue_id":"dear-8vd","depends_on_id":"dear-5py","type":"blocks","created_at":"2026-01-16T17:55:04.37903-05:00","created_by":"Cole Brown"}]}
{"id":"dear-90r","title":"Implement ShortConvolution module","description":"Implement conv.py with ShortConvolution - causal conv1d with configurable kernel size, SiLU activation, supports caching for inference. Similar to Mamba's depthwise conv.","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-16T10:14:55.425237-05:00","created_by":"Cole Brown","updated_at":"2026-01-16T10:23:09.828282-05:00","closed_at":"2026-01-16T10:23:09.828282-05:00","close_reason":"Closed","dependencies":[{"issue_id":"dear-90r","depends_on_id":"dear-207","type":"blocks","created_at":"2026-01-16T10:15:11.683849-05:00","created_by":"Cole Brown"}]}
{"id":"dear-9fx","title":"Epic: Implement Gated DeltaNet in JAX","description":"Implement Gated DeltaNet (ICLR 2025) in pure JAX/Equinox. Start with naive implementation, structure for future Pallas kernel optimization. Reference: NVlabs/GatedDeltaNet, fla-org/flash-linear-attention. Lives in jax-gated-deltanet/ submodule.\n\nSubtasks:\n- dear-207: Set up module structure and config\n- dear-r7d: Implement RMSNorm and FusedRMSNormGated  \n- dear-90r: Implement ShortConvolution\n- dear-6ym: Implement naive gated_delta_rule recurrence\n- dear-o3b: Implement GatedDeltaNetLayer\n- dear-aqd: Implement GatedDeltaNetBlock and Stack\n- dear-chh: Add tests","status":"closed","priority":2,"issue_type":"epic","owner":"bigswim@gmail.com","created_at":"2026-01-16T10:04:06.271336-05:00","created_by":"Cole Brown","updated_at":"2026-01-16T10:29:20.251255-05:00","closed_at":"2026-01-16T10:29:20.251255-05:00","close_reason":"Closed"}
{"id":"dear-9sn","title":"Create CLI entrypoint for probe evaluation","description":"CLI to run the full probe pipeline.\n\n## Usage\n```bash\nuv run python -m evals.voxceleb.probe \\\n    --checkpoint checkpoints-batch128-conservative/best \\\n    --voxceleb-root /path/to/voxceleb1 \\\n    --output results/voxceleb_probe.json\n```\n\n## Arguments\n- --checkpoint: Path to WavLeJEPA checkpoint\n- --voxceleb-root: Path to VoxCeleb1 dataset\n- --batch-size: Batch size for embedding extraction\n- --output: Where to save results JSON\n\n## Output\nJSON with accuracy metrics + config used.","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-16T17:54:59.630101-05:00","created_by":"Cole Brown","updated_at":"2026-01-17T12:32:57.907956-05:00","closed_at":"2026-01-17T12:32:57.907956-05:00","close_reason":"Implemented full CLI pipeline: load data, extract embeddings, train probe, save results","dependencies":[{"issue_id":"dear-9sn","depends_on_id":"dear-inc","type":"blocks","created_at":"2026-01-16T17:55:04.576602-05:00","created_by":"Cole Brown"}]}
{"id":"dear-adr","title":"Epic: Implement Speaker Attractor Generator","description":"Implement the Generator component as specified in docs/generator_design.md. This is the second stage of the speaker diarization pipeline that takes WavLeJEPA frame embeddings and produces speaker attractors.\n\n## Key Components\n1. Linear Attention Stack (Mamba-2/RWKV-7 style)\n2. Cross-Attention Module\n3. GRU-based Recurrent Generator\n4. Energy-based training objective\n5. Test-time optimization\n\n## Architecture Flow\n```\nWavLeJEPA (frozen) → Linear Attention Stack → Contextualized Frames\n                                                    ↓\n                                              Cross-Attention ← GRU hidden\n                                                    ↓\n                                              GRU → attractor + confidence\n                                              (loop until confidence \u003c θ)\n```\n\nSee docs/generator_design.md for full specification.","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-15T10:23:03.102164-05:00","created_by":"Cole Brown","updated_at":"2026-01-15T17:32:53.640306-05:00","closed_at":"2026-01-15T17:32:53.640306-05:00","close_reason":"Closed","labels":["epic"]}
{"id":"dear-aqd","title":"Implement GatedDeltaNetBlock and Stack","description":"Implement GatedDeltaNetBlock (pre-norm + GatedDeltaNetLayer + residual) and GatedDeltaNetStack (input projection + N blocks + final norm). Match generator/mamba.py patterns.","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-16T10:14:56.356155-05:00","created_by":"Cole Brown","updated_at":"2026-01-16T10:25:13.349373-05:00","closed_at":"2026-01-16T10:25:13.349373-05:00","close_reason":"Closed","dependencies":[{"issue_id":"dear-aqd","depends_on_id":"dear-o3b","type":"blocks","created_at":"2026-01-16T10:15:12.372794-05:00","created_by":"Cole Brown"}]}
{"id":"dear-bou","title":"Investigate and reduce encoder collapse/anisotropy","description":"# Investigate and reduce encoder collapse/anisotropy\n\nGoal: improve *encoder* (pre-projector) representation geometry in WavLeJEPA. Current diagnostics:\n- Context space near-collapsed (effective rank ~1.3, top1 eig ~95%)\n- Top-K space low-rank (eff rank ~4.9)\n- Projector space healthy (eff rank ~32)\n\n## Hypotheses\n1) Objective mismatch: SIGReg only in projector space allows encoder collapse.\n2) Instance norm + Top-K averaging washes out amplitude and drives low-rank alignment.\n3) Projector capacity too large (acts as adapter, letting encoder collapse).\n4) Data scale helps generalization but is secondary for geometry collapse.\n\n## Success criteria\n- Encoder/top-k effective rank increases substantially (target \u003e50 for d=768).\n- Random frame cosine mean approaches ~0 with std ~1/sqrt(d).\n- Verification EER improves *without* relying on projector space.\n- Improvements persist across ≥2 random seeds.\n\n## Experiment matrix (short runs first)\n\n### Phase 0 — Baseline instrumentation (already available)\n- Use `evals/voxceleb/collapse_check.py` to capture:\n  - effective rank + top1 eig ratio\n  - cosine mean/std and within vs between\n  - norm stats\n- Run for: feature_source={context, topk}, project={off,on}\n\n### Phase 1 — Loss/regularization changes (short runs, 10k–20k steps)\nA) Increase SIGReg weight:\n   - sigreg_weight: 0.02 (baseline) → 0.1 → 0.2\n   - Keep projector same\n   - Evaluate collapse metrics after run\nB) Reduce projector capacity:\n   - projector_hidden_dims: (2048,2048) → (512,) and/or (256,)\n   - projector_output_dim: 256 → 128\n   - Keep sigreg_weight constant\n   - Evaluate collapse metrics\nC) Move/duplicate SIGReg to encoder space (experimental):\n   - Option 1: apply SIGReg directly to context outputs (pre-projector)\n   - Option 2: add SIGReg on pre-projection embeddings in addition to projector\n   - Evaluate collapse metrics\n\n### Phase 2 — Representation extraction changes (no training or minimal retrain)\nD) Remove/modify instance norm in Top-K averaging:\n   - Disable instance norm in `ContextEncoder.forward_with_top_k`\n   - Or replace with LayerNorm\n   - Measure collapse metrics on existing checkpoint and on short retrain\nE) Pooling diagnostic (no retrain):\n   - Compare mean vs meanstd pooling on topk/context\n   - Confirm geometry issue is pre- or post-pooling\n\n### Phase 3 — Data scale sanity (optional)\nF) Small data scale sweep:\n   - Train short run on subset vs full dataset\n   - Check whether collapse changes materially with data diversity\n\n## Evaluation protocol per run\n- Run collapse_check on 100 files × 50 frames each (context/topk/projected).\n- Record effective_rank, top1_ratio, cosine stats, norm stats.\n- Run verification EER (evals.voxceleb.verify) with:\n  - pooling=mean, feature_source=topk\n  - pooling=meanstd, feature_source=context\n- Store all metrics in a single JSON for comparison.\n\n## Artifacts to capture per run\n- Config diff (sigreg_weight, projector dims, instance_norm on/off)\n- Training steps, data shard path, seed\n- Collapse metrics + EER\n- Qualitative notes\n\n## Out of scope (for now)\n- Full-scale data expansion\n- Downstream diarization tuning\n","acceptance_criteria":"Epic is fully specified with experiment matrix and evaluation criteria; provides enough detail to create runnable tasks later.","status":"open","priority":1,"issue_type":"epic","owner":"bigswim@gmail.com","created_at":"2026-01-20T22:46:50.005118-05:00","created_by":"Cole Brown","updated_at":"2026-01-20T22:46:50.005118-05:00","labels":["eval","research","training"]}
{"id":"dear-bou.1","title":"Add top-k normalization toggle","description":"Expose a config knob to control top-k normalization in ContextEncoder (instance, layer, none). Wire through model config and use in forward_with_top_k so we can run collapse diagnostics with/without instance norm.","acceptance_criteria":"Config supports top_k_norm={instance,layer,none}; ContextEncoder uses selected mode; inference/train paths unchanged except normalization choice; README or config comments updated.","status":"closed","priority":1,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-20T23:02:03.493312-05:00","created_by":"Cole Brown","updated_at":"2026-01-20T23:07:51.649151-05:00","closed_at":"2026-01-20T23:07:51.649151-05:00","close_reason":"Closed","dependencies":[{"issue_id":"dear-bou.1","depends_on_id":"dear-bou","type":"parent-child","created_at":"2026-01-20T23:02:03.49415-05:00","created_by":"Cole Brown"}]}
{"id":"dear-bou.2","title":"Add optional encoder-space SIGReg","description":"Allow SIGReg to be applied to pre-projector embeddings (context and/or predictions) in addition to projector space. Add config weights/flags and metrics so we can test if encoder collapse improves.","acceptance_criteria":"Loss supports encoder-space SIGReg toggles/weights; metrics report encoder-space sigreg terms; default behavior unchanged when disabled; training step compiles.","status":"closed","priority":1,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-20T23:02:17.565395-05:00","created_by":"Cole Brown","updated_at":"2026-01-20T23:08:05.968699-05:00","closed_at":"2026-01-20T23:08:05.968699-05:00","close_reason":"Closed","dependencies":[{"issue_id":"dear-bou.2","depends_on_id":"dear-bou","type":"parent-child","created_at":"2026-01-20T23:02:17.566351-05:00","created_by":"Cole Brown"}]}
{"id":"dear-bou.3","title":"Add collapse+EER comparison harness","description":"Create a small script/CLI that runs collapse_check (context/topk/projected) and VoxCeleb verification EER for a checkpoint, then writes a single JSON summary for easy run-to-run comparison.","acceptance_criteria":"CLI accepts checkpoint, voxceleb root, and output path; runs collapse_check + verify; output JSON includes metrics + config; README snippet or usage help.","status":"closed","priority":1,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-20T23:02:31.346442-05:00","created_by":"Cole Brown","updated_at":"2026-01-20T23:11:50.355532-05:00","closed_at":"2026-01-20T23:11:50.355532-05:00","close_reason":"Closed","dependencies":[{"issue_id":"dear-bou.3","depends_on_id":"dear-bou","type":"parent-child","created_at":"2026-01-20T23:02:31.347973-05:00","created_by":"Cole Brown"}]}
{"id":"dear-bou.4","title":"Remove projector; apply SIGReg directly to encoder embeddings","description":"Align with LeJEPA: remove projector module from WavLeJEPA and apply SIGReg directly to encoder embeddings (context/targets) used in prediction loss. Update configs/checkpointing accordingly.","acceptance_criteria":"Projector removed from model + checkpoints; SIGReg computed on encoder embeddings; training runs with sigreg_weight controlling encoder SIGReg; docs/configs updated; existing checkpoints handled or migration path documented.","status":"closed","priority":1,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-21T00:08:09.813722-05:00","created_by":"Cole Brown","updated_at":"2026-01-21T00:17:45.322488-05:00","closed_at":"2026-01-21T00:17:45.322488-05:00","close_reason":"Closed","dependencies":[{"issue_id":"dear-bou.4","depends_on_id":"dear-bou","type":"parent-child","created_at":"2026-01-21T00:08:09.814661-05:00","created_by":"Cole Brown"}]}
{"id":"dear-bwn","title":"Create synthetic multi-speaker test data","description":"Create synthetic test data for developing and testing the Generator.\n\n## Requirements\n- Generate fake 'frame embeddings' that simulate multi-speaker audio\n- Should have clear cluster structure (each speaker = distinct cluster)\n- Variable number of speakers (1-10)\n- Variable segment lengths per speaker\n- Should be reproducible (seeded RNG)\n\n## Use Cases\n- Unit testing energy functions\n- Integration testing full generator\n- Debugging training loop\n- Visualizing attractor quality\n\n## Suggested Approach\n```python\ndef make_synthetic_data(\n    num_speakers: int,\n    frames_per_speaker: int,\n    embedding_dim: int = 768,\n    noise_scale: float = 0.1,\n    key: PRNGKey\n) -\u003e tuple[Array, Array]:\n    \"\"\"\n    Returns:\n        frame_embeddings: [N, D] where N = num_speakers * frames_per_speaker\n        speaker_labels: [N] ground truth (for evaluation only)\n    \"\"\"\n```\n\n## Parent Epic\ndear-adr","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-15T10:25:30.054594-05:00","created_by":"Cole Brown","updated_at":"2026-01-15T11:24:55.622556-05:00","closed_at":"2026-01-15T11:24:55.622556-05:00","close_reason":"Closed","labels":["task"]}
{"id":"dear-c3w","title":"Epic: VoxCeleb Linear Probe Evaluation","description":"Implement speaker identification linear probe on VoxCeleb1 to evaluate WavLeJEPA representation quality.\n\n## Approach\n- Freeze WavLeJEPA encoder (stop gradients)\n- Extract frame embeddings → mean pool → utterance embedding\n- Train linear classifier (768 → num_speakers)\n- Report top-1/top-5 accuracy\n\n## Requirements\n- Specify checkpoint path from checkpoints dir\n- Isolated in `evals/` subfolder\n- Clean CLI interface\n\n## Success Criteria\n- 20-40% accuracy suggests representations capture speaker info\n- Lower accuracy doesn't invalidate model, just means aggregation matters","status":"closed","priority":2,"issue_type":"epic","owner":"bigswim@gmail.com","created_at":"2026-01-16T17:54:36.87485-05:00","created_by":"Cole Brown","updated_at":"2026-01-17T12:33:02.980791-05:00","closed_at":"2026-01-17T12:33:02.980791-05:00","close_reason":"Complete! Implemented VoxCeleb linear probe with data loading, embedding extraction, and evaluation."}
{"id":"dear-chh","title":"Add tests for jax-gated-deltanet","description":"Add tests: 1) gated_delta_rule matches expected output for simple case, 2) GatedDeltaNetLayer forward pass shapes, 3) Stack forward pass, 4) Gradient flow through all components.","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-16T10:14:56.615828-05:00","created_by":"Cole Brown","updated_at":"2026-01-16T10:29:20.249656-05:00","closed_at":"2026-01-16T10:29:20.249656-05:00","close_reason":"Closed","dependencies":[{"issue_id":"dear-chh","depends_on_id":"dear-aqd","type":"blocks","created_at":"2026-01-16T10:15:12.419114-05:00","created_by":"Cole Brown"}]}
{"id":"dear-d0d","title":"Implement embedding extraction from checkpoint","description":"Extract utterance embeddings from frozen WavLeJEPA checkpoint.\n\n## Requirements\n- Load checkpoint from specified path (Orbax format)\n- Stop gradients to encoder (jax.lax.stop_gradient)\n- Process: audio → WaveformEncoder → ContextEncoder → mean pool\n- Batch processing for efficiency\n\n## Interface\n```python\ndef extract_embeddings(\n    checkpoint_path: Path,\n    audio_paths: list[Path],\n    batch_size: int = 32,\n) -\u003e np.ndarray:  # [num_utterances, 768]\n```","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-16T17:54:58.730378-05:00","created_by":"Cole Brown","updated_at":"2026-01-17T12:31:36.486491-05:00","closed_at":"2026-01-17T12:31:36.486491-05:00","close_reason":"Implemented embedding extraction with frozen encoder and mean pooling","dependencies":[{"issue_id":"dear-d0d","depends_on_id":"dear-5py","type":"blocks","created_at":"2026-01-16T17:55:04.430714-05:00","created_by":"Cole Brown"}]}
{"id":"dear-eba","title":"Update LinearAttentionStack factory logic","description":"Check which SSM config is present and instantiate Mamba2Block or GatedDeltaNetBlock accordingly. Import GatedDeltaNetBlock from jax_gated_deltanet.","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-16T11:24:03.346639-05:00","created_by":"Cole Brown","updated_at":"2026-01-16T11:28:40.539747-05:00","closed_at":"2026-01-16T11:28:40.539747-05:00","close_reason":"Closed","dependencies":[{"issue_id":"dear-eba","depends_on_id":"dear-2x8","type":"blocks","created_at":"2026-01-16T11:24:09.569272-05:00","created_by":"Cole Brown"},{"issue_id":"dear-eba","depends_on_id":"dear-pak","type":"blocks","created_at":"2026-01-16T11:24:10.072143-05:00","created_by":"Cole Brown"}]}
{"id":"dear-ekt","title":"Implement intra-chunk delta rule computation","description":"Compute outputs within each chunk using the WY representation. This handles the O(chunk²) parallel work within each chunk.\n\n- Compute w, u from WY representation  \n- Compute local (diagonal) outputs\n- Handle the delta rule update in parallel within chunk","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-16T10:59:51.486369-05:00","created_by":"Cole Brown","updated_at":"2026-01-16T11:13:36.061923-05:00","closed_at":"2026-01-16T11:13:36.061923-05:00","close_reason":"Closed","dependencies":[{"issue_id":"dear-ekt","depends_on_id":"dear-gnt","type":"blocks","created_at":"2026-01-16T11:00:13.406068-05:00","created_by":"Cole Brown"}]}
{"id":"dear-gnt","title":"Implement chunk utilities and WY representation","description":"Implement helper functions for chunkwise algorithm:\n- segment sum (stable cumsum for decay computation)\n- solve_tril (triangular solve for WY representation)\n- chunk reshaping utilities\n- A matrix computation (scaled k @ k.T with gating)\n\nReference: FLA's chunk_scaled_dot_kkt, solve_tril, wy_fast.py","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-16T10:59:46.065478-05:00","created_by":"Cole Brown","updated_at":"2026-01-16T11:13:36.046772-05:00","closed_at":"2026-01-16T11:13:36.046772-05:00","close_reason":"Closed"}
{"id":"dear-hmz","title":"Extract Mamba2Config from GeneratorConfig","description":"Create Mamba2Config dataclass with state_size, chunk_size, time_step_*, A_initializer_range params. Remove from GeneratorConfig top level.","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-16T11:24:02.684707-05:00","created_by":"Cole Brown","updated_at":"2026-01-16T11:26:22.00447-05:00","closed_at":"2026-01-16T11:26:22.00447-05:00","close_reason":"Closed","dependencies":[{"issue_id":"dear-hmz","depends_on_id":"dear-2x8","type":"blocks","created_at":"2026-01-16T11:24:09.434652-05:00","created_by":"Cole Brown"}]}
{"id":"dear-ile","title":"Implement test-time optimization loop","description":"Implement attractor refinement via energy minimization at inference.\n\n## Specification\n```python\ndef refine_attractors(A_init, X, num_steps=50, lr=0.01):\n    A = A_init\n    for _ in range(num_steps):\n        grad_A = grad(E, argnums=0)(A, X)\n        A = A - lr * grad_A\n    return A\n```\n\n## Requirements\n- Gradient descent on energy w.r.t. attractors only\n- Configurable: num_steps, learning_rate\n- Consider stopping criteria based on energy convergence\n- May need attractor re-normalization after gradient steps\n- Must handle masking for invalid attractors\n\n## Parent Epic\ndear-adr","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-15T10:25:14.661784-05:00","created_by":"Cole Brown","updated_at":"2026-01-15T11:30:46.039708-05:00","closed_at":"2026-01-15T11:30:46.039708-05:00","close_reason":"Closed","labels":["task"]}
{"id":"dear-inc","title":"Implement linear probe training and evaluation","description":"Train and evaluate linear classifier on extracted embeddings.\n\n## Requirements\n- Split embeddings into train/test (80/20 stratified by speaker)\n- Train LogisticRegression or single Linear layer\n- Report: top-1 accuracy, top-5 accuracy, per-class breakdown\n\n## Interface\n```python\ndef train_linear_probe(\n    embeddings: np.ndarray,  # [N, 768]\n    labels: np.ndarray,      # [N]\n    test_size: float = 0.2,\n) -\u003e dict:  # {\"top1\": float, \"top5\": float, ...}\n```","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-16T17:54:59.187455-05:00","created_by":"Cole Brown","updated_at":"2026-01-17T12:32:21.936906-05:00","closed_at":"2026-01-17T12:32:21.936906-05:00","close_reason":"Implemented linear probe with train/test split and optional cross-validation","dependencies":[{"issue_id":"dear-inc","depends_on_id":"dear-d0d","type":"blocks","created_at":"2026-01-16T17:55:04.480652-05:00","created_by":"Cole Brown"},{"issue_id":"dear-inc","depends_on_id":"dear-8vd","type":"blocks","created_at":"2026-01-16T17:55:04.52774-05:00","created_by":"Cole Brown"}]}
{"id":"dear-kgw","title":"Implement energy functions","description":"Implement the energy-based objective functions for training.\n\n## Energy Components\n\n### E_assignment (Frame-to-Attractor)\n```python\nd_ik = ||x_i - a_k||²                 # L2 squared distance\nw_ik = softmax(-d_ik / τ)            # soft assignment (softmin)\nE_assignment = (1/N) * Σ_i Σ_k w_ik * d_ik\n```\n\n### E_separation (Attractor Diversity)\n```python\nE_separation = Σ_{k≠j} max(0, margin - ||a_k - a_j||)\n```\nHinge loss with configurable margin.\n\n### E_coverage (No Orphan Frames) - Optional\n```python\nusage_k = Σ_i w_ik\nE_coverage = Σ_k max(0, min_usage - usage_k)\n```\n\n## Combined Energy\n```python\nE(A, X) = E_assignment + λ_sep * E_separation + λ_cov * E_coverage\n```\n\n## Requirements\n- All functions must be JAX-differentiable\n- Temperature τ should be a parameter (for annealing)\n- Must handle variable number of valid attractors (masking)\n\n## Parent Epic\ndear-adr","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-15T10:24:56.402514-05:00","created_by":"Cole Brown","updated_at":"2026-01-15T11:12:49.924457-05:00","closed_at":"2026-01-15T11:12:49.924457-05:00","close_reason":"Closed","labels":["task"]}
{"id":"dear-lsn","title":"Add equivalence tests for chunkwise vs naive","description":"Validate chunkwise implementation matches naive scan:\n- Test output equivalence (allclose with tolerance)\n- Test final state equivalence\n- Test with various seq lengths (divisible, non-divisible by chunk_size)\n- Test with/without initial state\n- Test gradient equivalence\n- Benchmark comparison (optional)","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-16T11:00:08.379286-05:00","created_by":"Cole Brown","updated_at":"2026-01-16T11:13:36.101843-05:00","closed_at":"2026-01-16T11:13:36.101843-05:00","close_reason":"Closed","dependencies":[{"issue_id":"dear-lsn","depends_on_id":"dear-ycg","type":"blocks","created_at":"2026-01-16T11:00:13.698417-05:00","created_by":"Cole Brown"}]}
{"id":"dear-mfk","title":"Implement CrossAttention module","description":"Implement multi-head cross-attention for querying contextualized frames from GRU hidden state.\n\n## Requirements\n- Multi-head attention (configurable num_heads)\n- Query: GRU hidden state [D]\n- Key/Value: contextualized frames [N, D]\n- Output: attended context vector [D]\n\n## Interface\n```python\nclass CrossAttention(eqx.Module):\n    num_heads: int\n    head_dim: int\n    w_q: eqx.nn.Linear\n    w_k: eqx.nn.Linear\n    w_v: eqx.nn.Linear\n    w_o: eqx.nn.Linear\n    \n    def __call__(self, query: Array, kv: Array) -\u003e Array:\n        \"\"\"\n        Args:\n            query: [D] GRU hidden state\n            kv: [N, D] contextualized frames\n        Returns:\n            [D] attended context vector\n        \"\"\"\n```\n\n## Parent Epic\ndear-adr","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-15T10:24:29.164819-05:00","created_by":"Cole Brown","updated_at":"2026-01-15T11:01:04.73503-05:00","closed_at":"2026-01-15T11:01:04.73503-05:00","close_reason":"Closed","labels":["task"]}
{"id":"dear-mhy","title":"Implement LinearAttentionStack","description":"Implement the stack of gated linear attention layers that contextualizes frame embeddings.\n\n## Requirements\n- Stack 4-6 LinearAttentionLayer modules\n- Configurable via GeneratorConfig.num_layers\n- Maintains sequence length (no pooling)\n\n## Interface\n```python\nclass LinearAttentionStack(eqx.Module):\n    layers: list[LinearAttentionLayer]\n    \n    def __call__(self, x: Array) -\u003e Array:\n        \"\"\"Process frame embeddings through all layers.\n        \n        Args:\n            x: [N, D] frame embeddings\n        Returns:\n            [N, D] contextualized frame representations\n        \"\"\"\n```\n\n## Dependencies\n- Requires LinearAttentionLayer implementation\n\n## Parent Epic\ndear-adr","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-15T10:24:18.737134-05:00","created_by":"Cole Brown","updated_at":"2026-01-15T10:58:01.628143-05:00","closed_at":"2026-01-15T10:58:01.628143-05:00","close_reason":"Closed","labels":["task"]}
{"id":"dear-ngw","title":"Implement GeneratorConfig dataclass","description":"Implement the configuration dataclass for the Generator.\n\n## Fields\n```python\n@dataclass\nclass GeneratorConfig:\n    # Dimensions\n    input_dim: int = 768          # WavLeJEPA output dim\n    hidden_dim: int = 768         # GRU hidden size\n    attractor_dim: int = 768      # Output attractor dimension\n    \n    # Linear attention stack\n    num_layers: int = 4           # Number of linear attention layers\n    \n    # Cross-attention\n    num_heads: int = 8            # Multi-head attention heads\n    \n    # Generation\n    max_attractors: int = 10      # Maximum speakers\n    confidence_threshold: float = 0.5\n    \n    # Energy weights\n    lambda_separation: float = 1.0\n    lambda_coverage: float = 0.1\n    separation_margin: float = 1.0\n    \n    # Temperature annealing\n    tau_start: float = 1.0\n    tau_end: float = 0.1\n    \n    # Confidence training\n    usage_threshold: float = 0.5  # seconds\n```\n\n## Parent Epic\ndear-adr","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-15T10:25:19.793439-05:00","created_by":"Cole Brown","updated_at":"2026-01-15T10:59:37.268067-05:00","closed_at":"2026-01-15T10:59:37.268067-05:00","close_reason":"Closed","labels":["task"]}
{"id":"dear-o3b","title":"Implement GatedDeltaNetLayer","description":"Implement layers.py GatedDeltaNetLayer: q/k/v/a/b projections, short convs on q/k/v, L2 norm on q/k, call gated_delta_rule, output gating with FusedRMSNormGated, output projection. Follow FLA structure.","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-16T10:14:56.078846-05:00","created_by":"Cole Brown","updated_at":"2026-01-16T10:25:13.347804-05:00","closed_at":"2026-01-16T10:25:13.347804-05:00","close_reason":"Closed","dependencies":[{"issue_id":"dear-o3b","depends_on_id":"dear-r7d","type":"blocks","created_at":"2026-01-16T10:15:12.025656-05:00","created_by":"Cole Brown"},{"issue_id":"dear-o3b","depends_on_id":"dear-90r","type":"blocks","created_at":"2026-01-16T10:15:12.066394-05:00","created_by":"Cole Brown"},{"issue_id":"dear-o3b","depends_on_id":"dear-6ym","type":"blocks","created_at":"2026-01-16T10:15:12.107166-05:00","created_by":"Cole Brown"}]}
{"id":"dear-ow8","title":"Implement multiple target groups in predictor","description":"## Problem\n\nWavLeJEPA currently uses a single target mask where all target tokens can attend to each other during prediction. WavJEPA uses multiple target groups (default 4) where each group is predicted independently - targets in group 1 cannot attend to targets in groups 2, 3, 4.\n\n### Current behavior (WavLeJEPA)\n- One context mask (~10% of frames)\n- One combined target mask (~20% of frames)  \n- Single predictor forward pass\n- All target tokens can attend to all other target tokens\n\n### Expected behavior (WavJEPA)\n- One context mask\n- Multiple target masks (e.g., 4 disjoint regions)\n- Predictor sees context + ONE target group at a time\n- Target groups are isolated - prevents shortcut learning where targets copy from nearby targets instead of predicting from context\n\n## Why this matters\n\n1. **Prevents shortcuts**: Targets attending to each other may allow the model to \"cheat\" by sharing information between nearby target positions rather than truly predicting from context\n2. **Richer training signal**: Each target group provides an independent prediction task from the same context\n3. **Matches reference**: WavJEPA explicitly isolates target groups\n\n## Proposed solution\n\n### 1. Masking changes (model.py)\n\nSample multiple target masks instead of one:\n\n```python\n# Sample N target masks, each disjoint from context and each other\nnum_target_groups = 4\ntarget_masks = []\nused_positions = context_mask.copy()\nfor _ in range(num_target_groups):\n    target_mask = sample_target_mask(seq_len, used_positions, config.masking, key)\n    target_masks.append(target_mask)\n    used_positions = used_positions | target_mask\n```\n\n### 2. Predictor changes (predictor.py)\n\nOption A - Batched (efficient, matches WavJEPA):\n```python\n# Repeat context for each target group, batch together\n# context_proj: [ctx_len, dim] -\u003e [num_groups, ctx_len, dim]\n# Each group gets its own mask tokens and attention mask\n# Use vmap over groups for efficiency\n\ndef __call__(self, context_output, context_positions, target_positions_per_group, ...):\n    # target_positions_per_group: [num_groups, max_targets]\n    # num_valid_per_group: [num_groups]\n    \n    def predict_single_group(target_positions, num_targets):\n        # Build attention mask for context + this target group only\n        ...\n        return predictions\n    \n    # vmap over target groups\n    all_predictions = jax.vmap(predict_single_group)(\n        target_positions_per_group, \n        num_valid_per_group\n    )\n    return all_predictions  # [num_groups, max_targets, dim]\n```\n\nOption B - Sequential (simpler but slower):\n```python\n# Loop over target groups, concatenate results\npredictions = []\nfor i in range(num_target_groups):\n    pred = self.predict_single_group(context, target_positions[i], ...)\n    predictions.append(pred)\n```\n\n### 3. Loss changes (losses.py)\n\nAggregate loss over all target groups:\n```python\n# predictions: [num_groups, max_targets, dim]\n# targets: [num_groups, max_targets, dim]  \n# num_valid: [num_groups]\nper_group_loss = jax.vmap(masked_invariance_loss_single)(predictions, targets, num_valid)\ntotal_loss = jnp.mean(per_group_loss)\n```\n\n## Implementation notes\n\n- Use `jax.vmap` over target groups for efficiency - avoids Python loop overhead\n- The context encoding is shared, only predictor runs multiple times (batched via vmap)\n- Memory scales with num_target_groups for predictor activations\n- Consider making num_target_groups configurable in MaskingConfig\n\n## Acceptance criteria\n\n- [ ] Multiple target masks sampled (configurable count, default 4)\n- [ ] Target groups are disjoint from each other and from context\n- [ ] Predictor processes each group with isolated attention\n- [ ] Loss aggregates correctly over all groups\n- [ ] Implementation uses vmap for efficiency\n- [ ] Training runs without OOM at current batch sizes","status":"open","priority":2,"issue_type":"feature","owner":"cole@kiosk.app","created_at":"2026-01-23T01:35:14.708908352Z","created_by":"Cole Brown","updated_at":"2026-01-23T01:35:14.708908352Z"}
{"id":"dear-pak","title":"Update GeneratorConfig with nested SSM configs","description":"Add Optional[Mamba2Config] and Optional[GatedDeltaNetConfig] fields. Add validation: exactly one must be provided. Update property methods.","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-16T11:24:03.038816-05:00","created_by":"Cole Brown","updated_at":"2026-01-16T11:26:22.01906-05:00","closed_at":"2026-01-16T11:26:22.01906-05:00","close_reason":"Closed","dependencies":[{"issue_id":"dear-pak","depends_on_id":"dear-2x8","type":"blocks","created_at":"2026-01-16T11:24:09.503174-05:00","created_by":"Cole Brown"},{"issue_id":"dear-pak","depends_on_id":"dear-hmz","type":"blocks","created_at":"2026-01-16T11:24:10.012373-05:00","created_by":"Cole Brown"}]}
{"id":"dear-r7d","title":"Implement RMSNorm and FusedRMSNormGated","description":"Implement norm.py with RMSNorm (reuse from generator/) and FusedRMSNormGated (RMSNorm that takes a gate tensor and multiplies output by sigmoid(gate)).","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-16T10:14:55.154734-05:00","created_by":"Cole Brown","updated_at":"2026-01-16T10:23:09.826459-05:00","closed_at":"2026-01-16T10:23:09.826459-05:00","close_reason":"Closed","dependencies":[{"issue_id":"dear-r7d","depends_on_id":"dear-207","type":"blocks","created_at":"2026-01-16T10:15:11.632547-05:00","created_by":"Cole Brown"}]}
{"id":"dear-tj6","title":"Diarization eval baseline (2B)","description":"Build a baseline diarization evaluation pipeline to measure DER/JER using WavLeJEPA frame-level embeddings. Scope: dataset selection (AMI/DIHARD/CallHome TBD), frame extraction, clustering (AHC or spectral), VAD handling (oracle first), and metrics report.","acceptance_criteria":"Chosen dataset documented; CLI runs end-to-end; reports DER/JER (with oracle VAD at minimum); results saved to JSON + brief usage notes.","status":"open","priority":3,"issue_type":"epic","owner":"bigswim@gmail.com","created_at":"2026-01-20T21:26:55.05612-05:00","created_by":"Cole Brown","updated_at":"2026-01-20T22:48:57.678303-05:00","labels":["diarization","eval"]}
{"id":"dear-tj6.1","title":"Research EEND-EDA / EEND-TA eval protocols","description":"Use Exa web search to summarize EEND-EDA and EEND-TA evaluation protocols: datasets used, preprocessing/VAD assumptions, metrics (DER/JER), and any reported baselines. Capture any implementation details needed to align our diarization eval with the papers.","acceptance_criteria":"Short writeup with citations/links; clear list of datasets, metrics, and protocol choices; recommendations for aligning our DIHARD eval.","notes":"Research summary: EEND-TA paper (arXiv:2312.06253) uses simulated LibriSpeech mixtures for pretraining; fine-tunes/evals on DIHARD III, VoxConverse, MagicData-RAMC, AMI Mix, AMI SDM1; reports DER as primary metric. EEND-EDA paper (arXiv:2005.09921) explicitly reports CALLHOME (2-spk + unknown spk); AMI/DIHARD appear more in later EEND variants. DIHARD official scoring reports DER/JER (core/full). For alignment: DIHARD III + official scorer, report DER/JER; start with oracle VAD.","status":"open","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-20T21:30:19.059652-05:00","created_by":"Cole Brown","updated_at":"2026-01-20T21:33:12.312836-05:00","labels":["diarization","eval","research"],"dependencies":[{"issue_id":"dear-tj6.1","depends_on_id":"dear-tj6","type":"parent-child","created_at":"2026-01-20T21:30:19.061749-05:00","created_by":"Cole Brown"}]}
{"id":"dear-wk2","title":"Set up training loop with temperature annealing","description":"Implement the training loop for the Generator with deterministic annealing.\n\n## Requirements\n- Training objective: L = E(A, X) + λ_conf * L_confidence\n- Temperature annealing: τ goes from tau_start (soft) to tau_end (hard) over training\n- Frozen WavLeJEPA encoder (no gradients)\n- Trainable: LinearAttentionStack, CrossAttention, GRU, output heads\n\n## Training Flow\n1. Encode audio with frozen WavLeJEPA\n2. Pass through linear attention stack\n3. Generate attractors with GRU\n4. Compute energy loss + confidence loss\n5. Backprop through generator only\n\n## Considerations\n- Learning rate schedule\n- Gradient clipping\n- Checkpointing\n- Logging (loss components, attractor counts, etc.)\n\n## Parent Epic\ndear-adr","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-15T10:25:24.926136-05:00","created_by":"Cole Brown","updated_at":"2026-01-15T17:27:46.727641-05:00","closed_at":"2026-01-15T17:27:46.727641-05:00","close_reason":"Closed","labels":["task"]}
{"id":"dear-ycg","title":"Wire up chunkwise gated_delta_rule_chunk function","description":"Combine all components into gated_delta_rule_chunk():\n- Same signature as gated_delta_rule (drop-in replacement)\n- Chunk input sequences\n- Call intra-chunk computation\n- Call inter-chunk propagation  \n- Combine outputs and reshape back\n- Handle edge cases (non-divisible seq len, initial state)","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-16T11:00:03.234399-05:00","created_by":"Cole Brown","updated_at":"2026-01-16T11:13:36.088828-05:00","closed_at":"2026-01-16T11:13:36.088828-05:00","close_reason":"Closed","dependencies":[{"issue_id":"dear-ycg","depends_on_id":"dear-ekt","type":"blocks","created_at":"2026-01-16T11:00:13.553837-05:00","created_by":"Cole Brown"},{"issue_id":"dear-ycg","depends_on_id":"dear-7ac","type":"blocks","created_at":"2026-01-16T11:00:13.625004-05:00","created_by":"Cole Brown"}]}
{"id":"dear-yne","title":"Implement confidence head training loss","description":"Implement the auxiliary usage-based confidence loss.\n\n## Specification\n```python\nusage_k = Σ_i w_ik                           # total soft-assignment mass\ntarget_k = 1 if usage_k \u003e threshold else 0   # threshold ≈ 0.5s audio\nL_confidence = (1/K) * Σ_k BCE(c_k, target_k)\n```\n\n## Requirements\n- Binary cross-entropy loss\n- Usage threshold configurable (default: frames equivalent to 0.5s audio)\n- Must handle masking for invalid attractors\n- Ensures confidence is self-consistent with energy function\n\n## Dependencies\n- Soft assignment weights from energy function (w_ik)\n\n## Parent Epic\ndear-adr","status":"closed","priority":2,"issue_type":"task","owner":"bigswim@gmail.com","created_at":"2026-01-15T10:25:09.531043-05:00","created_by":"Cole Brown","updated_at":"2026-01-15T11:24:50.546459-05:00","closed_at":"2026-01-15T11:24:50.546459-05:00","close_reason":"Closed","labels":["task"]}
{"id":"dear-ysj","title":"Align masking ratios with WavJEPA and add retry mechanism","description":"## Problem\n\nWavLeJEPA's masking configuration differs from WavJEPA in two ways:\n\n1. **Different effective ratios**: WavJEPA targets ~35% context, ~23% targets. WavLeJEPA produces ~48% context, ~11% targets.\n2. **No minimum coverage guarantee**: WavJEPA resamples if context falls below 10%. WavLeJEPA can produce degenerate samples with very low context.\n\n### Current behavior\n\n```python\n@dataclass\nclass MaskingConfig:\n    context_prob: float = 0.065  # Probability of starting a context block\n    context_length: int = 10\n    target_prob: float = 0.025\n    target_length: int = 10\n```\n\nThis produces:\n- Context: ~48% (range 34-57%)\n- Target: ~11% (range 2-16%)\n- No retry on low coverage\n\n### Desired behavior\n\nMatch WavJEPA's effective ratios:\n- Context: ~35%\n- Target: ~23%\n- Minimum context guarantee: 10%\n\n## Proposed solution\n\n### 1. Add ratio-based configuration\n\nReplace probability-based config with explicit ratio targets:\n\n```python\n@dataclass\nclass MaskingConfig:\n    # Context configuration\n    context_ratio: float = 0.35      # Target ratio of sequence as context\n    context_block_length: int = 10   # Length of each context block\n    min_context_ratio: float = 0.10  # Minimum acceptable context ratio\n    \n    # Target configuration  \n    target_ratio: float = 0.23       # Target ratio of sequence as targets\n    target_block_length: int = 10    # Length of each target block\n```\n\n### 2. Implement deterministic ratio sampling\n\nInstead of bernoulli sampling with a probability, sample to hit a target ratio:\n\n```python\ndef sample_context_mask(seq_len: int, config: MaskingConfig, key: PRNGKeyArray):\n    target_frames = int(seq_len * config.context_ratio)\n    num_blocks = target_frames // config.context_block_length\n    \n    # Sample num_blocks random start positions\n    # Expand each to block_length frames\n    # Handle overlaps\n    ...\n```\n\n### 3. Add retry mechanism\n\nWrap sampling in a retry loop:\n\n```python\ndef sample_masks(seq_len: int, config: MaskingConfig, key: PRNGKeyArray):\n    max_retries = 10\n    \n    for attempt in range(max_retries):\n        key, k1, k2 = jax.random.split(key, 3)\n        \n        context_mask = sample_context_mask(seq_len, config, k1)\n        target_mask = sample_target_mask(seq_len, context_mask, config, k2)\n        \n        # Remove overlap (targets take priority)\n        context_mask = context_mask \u0026 ~target_mask\n        \n        context_ratio = jnp.sum(context_mask) / seq_len\n        if context_ratio \u003e= config.min_context_ratio:\n            return context_mask, target_mask\n    \n    # Fallback: return last attempt with warning\n    return context_mask, target_mask\n```\n\n### 4. JAX-compatible retry (for JIT)\n\nSince JAX JIT doesn't support Python while loops with dynamic conditions, use `jax.lax.while_loop`:\n\n```python\ndef sample_masks_jit(seq_len: int, config: MaskingConfig, key: PRNGKeyArray):\n    def cond_fn(state):\n        context_mask, _, _, attempt = state\n        context_ratio = jnp.sum(context_mask) / seq_len\n        return (context_ratio \u003c config.min_context_ratio) \u0026 (attempt \u003c max_retries)\n    \n    def body_fn(state):\n        _, _, key, attempt = state\n        key, k1, k2 = jax.random.split(key, 3)\n        context_mask = sample_context_mask(seq_len, config, k1)\n        target_mask = sample_target_mask(seq_len, context_mask, config, k2)\n        context_mask = context_mask \u0026 ~target_mask\n        return (context_mask, target_mask, key, attempt + 1)\n    \n    init_state = (initial_context, initial_target, key, 0)\n    final_state = jax.lax.while_loop(cond_fn, body_fn, init_state)\n    return final_state[0], final_state[1]\n```\n\n## Implementation notes\n\n- Keep the block-based sampling approach (not inverse masking) for clarity\n- Ratio-based config is more intuitive than probability-based\n- The retry mechanism ensures training stability\n- Use `jax.lax.while_loop` for JIT compatibility\n- Consider logging/metrics for retry frequency to detect config issues\n\n## Acceptance criteria\n\n- [ ] MaskingConfig uses ratio-based parameters (context_ratio, target_ratio)\n- [ ] Default ratios match WavJEPA (~35% context, ~23% target)\n- [ ] Retry mechanism resamples if context \u003c min_context_ratio\n- [ ] Retry is JIT-compatible via jax.lax.while_loop\n- [ ] Existing tests pass with new config\n- [ ] Document the config parameters","status":"closed","priority":2,"issue_type":"feature","owner":"cole@kiosk.app","created_at":"2026-01-23T02:04:16.28666286Z","created_by":"Cole Brown","updated_at":"2026-01-23T02:30:28.105718161Z","closed_at":"2026-01-23T02:30:28.105718161Z","close_reason":"Closed"}
